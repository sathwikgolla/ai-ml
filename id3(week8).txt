from collections import Counter
import math
from pprint import pprint
#Entropy calculation function
def entropy(probs):
return sum(-prob * math.log(prob, 2) for prob in probs if prob > 0)
def entropy_of_list(a_list):
cnt=Counter(a_list)
num_instances=len(a_list)
probs=[x/num_instances for x in cnt.values()]
return entropy(probs)
# Corrected Information Gain function
def information_gain(df, split_attribute_name, target_attribute_name):
df_split = df.groupby(split_attribute_name)
nobs = len(df.index) * 1.0
df_agg_ent = df_split[target_attribute_name].agg([entropy_of_list, lambda x:len(x) / nobs])
df_agg_ent.columns = ['entropy', 'prob']
avg_info = sum(df_agg_ent['entropy'] * df_agg_ent['prob'])
old_entropy = entropy_of_list(df[target_attribute_name])
return old_entropy - avg_info
#ID3 Decision Tree algorithm
def id3DT(df,target_attribute_name, attribute_names, default_class=None):#PlayTennis
cnt = Counter(df[target_attribute_name])#yes:9,no:5
if len(cnt) == 1:
return next(iter(cnt))
elif df.empty or not attribute_names:
return default_class
else:
default_class = max(cnt, key=cnt.get)#yes
gain = [information_gain(df, attr, target_attribute_name) for attr in attribute_names]
index_of_max = gain.index(max(gain))#0.2464
best_attr = attribute_names[index_of_max]#outlook
tree = {best_attr: {}}
remaining_attributes = [i for i in attribute_names if i != best_attr]
for attr_val, data_subset in df.groupby(best_attr):
subtree = id3DT(data subset, target attribute name, remaining attributes, default class)
import pandas as pd

Next steps:
The Resultant Decision Tree is:
{'Outlook': {'Overcast': 'Yes',
'Rain': {'Wind': {'Strong': 'No', 'Weak': 'Yes'}},
'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}}}
Outlook Temperature Humidity Wind Predicted
0 Rain Mild High Weak Yes
1 Sunny Hot Normal Strong Yes

( _ , g _ _ , g_ , _ )

tree[best_attr][attr_val] = subtree
return tree
tree
def classify(instance,tree,default=None):
attribute=next(iter(tree))
if instance[attribute] in tree[attribute]:
result=tree[attribute][instance[attribute]]
if isinstance(result,dict):
return classify(instance,result)
else:
return result
else:
return default
data={
'Outlook':['Sunny','Sunny','Overcast','Rain','Rain','Rain','Overcast','Sunny','Sunny','Rain','Sunny','Overcast','Overcast','Rain'],
'Temperature':['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild'],
'Humidity':['High','High','High','High','Normal','Normal','Normal','High','Normal','Normal','Normal','High','Normal','High'],
'Wind':['Weak','Strong','Weak','Weak','Weak','Strong','Strong','Weak','Weak','Weak','Strong','Strong','Weak','Strong'],
'PlayTennis':['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']
}
df=pd.DataFrame(data)
df
attribute_names=list(df.columns)
attribute_names.remove('PlayTennis')
attribute_names
tree=id3DT(df,'PlayTennis',attribute_names)
print("The Resultant Decision Tree is:")
pprint(tree)
new_data={
'Outlook':['Rain','Sunny'],
'Temperature':['Mild','Hot'],
'Humidity':['High','Normal'],
'Wind':['Weak','Strong']
}
df2=pd.DataFrame(new_data)
df2
df2['Predicted']=df2.apply(classify,axis=1,args=(tree,'No'))